{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3317252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94abff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\kanin\\Desktop\\CV\\Neural network\")\n",
    "import core.nn as nn\n",
    "import core.optim as optim\n",
    "from core.losses import MSE, CrossEntropy\n",
    "from core.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453e955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(loader_fn, train_num, test_num):\n",
    "    data_x, data_y = loader_fn\n",
    "    \n",
    "    classes = np.unique(data_y)\n",
    "    \n",
    "    train_x_list = []\n",
    "    train_y_list = []\n",
    "    test_x_list = []\n",
    "    test_y_list = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_indices = np.where(data_y == cls)[0]\n",
    "        cls_indices = np.random.permutation(cls_indices)\n",
    "        \n",
    "        X_cls = data_x[cls_indices]\n",
    "        Y_cls = data_y[cls_indices]\n",
    "        \n",
    "        train_x_list.append(X_cls[:train_num])\n",
    "        train_y_list.append(Y_cls[:train_num])\n",
    "        \n",
    "        test_x_list.append(X_cls[train_num:train_num + test_num])\n",
    "        test_y_list.append(Y_cls[train_num:train_num + test_num])\n",
    "        \n",
    "    X_train = np.concatenate(train_x_list)\n",
    "    y_train = np.concatenate(train_y_list)\n",
    "    X_test = np.concatenate(test_x_list)\n",
    "    y_test = np.concatenate(test_y_list)\n",
    "    \n",
    "    train_perm = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[train_perm]\n",
    "    y_train = y_train[train_perm]\n",
    "    \n",
    "    test_perm = np.random.permutation(len(X_test))\n",
    "    X_test = X_test[test_perm]\n",
    "    y_test = y_test[test_perm]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d590209",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4515062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "# Normalize pixel values to range 0-1 by dividing by 255.0 (grayscale images originally 0-255)\n",
    "# Convert labels to integer type using astype('int16') to ensure numeric operations work correctly \n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset(\n",
    "    (\n",
    "        np.asarray(data[\"data\"].values) / 255.0,                 # Normalize input images\n",
    "        np.asarray(data[\"target\"].values.astype('int16'))        # Convert labels to integers\n",
    "    ),\n",
    "    train_num=30,\n",
    "    test_num=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c29154",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8429821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, pool_size=(2,2), stride=(1,1)):\n",
    "        super().__init__()\n",
    "        # Ensure pool_size and stride are tuples (height, width)\n",
    "        if isinstance(pool_size, int):\n",
    "            pool_size = (pool_size, pool_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "\n",
    "        self.pool_size_y, self.pool_size_x = pool_size  # pooling window height and width\n",
    "        self.stride_y, self.stride_x = stride          # stride in vertical and horizontal directions\n",
    "        self.x = None     # cache input for backward pass\n",
    "        self.mask = None  # cache mask for backward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x      # input array of shape (N, C, H, W)\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # Calculate output height and width\n",
    "        H_out = (H - self.pool_size_y) // self.stride_y + 1\n",
    "        W_out = (W - self.pool_size_x) // self.stride_x + 1\n",
    "\n",
    "        # Initialize output array and mask\n",
    "        out = np.zeros((N, C, H_out, W_out))\n",
    "        self.mask = np.zeros_like(x, dtype=int)  # shape (N, C, H, W)\n",
    "\n",
    "        # Loop over batch, channels, and output spatial dimensions\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        # Determine window start and end indices\n",
    "                        start_i = i * self.stride_y\n",
    "                        start_j = j * self.stride_x\n",
    "\n",
    "                        # Extract region of input corresponding to the pooling window\n",
    "                        region = x[n, c, start_i:start_i+self.pool_size_y,\n",
    "                                   start_j:start_j+self.pool_size_x]\n",
    "\n",
    "                        # Find max value in window\n",
    "                        max_val = np.max(region)\n",
    "                        out[n, c, i, j] = max_val\n",
    "\n",
    "                        # Create local mask (1 where max, 0 elsewhere)\n",
    "                        local_mask = (region == max_val).astype(int)\n",
    "\n",
    "                        # Store local mask in the global mask\n",
    "                        self.mask[n, c, start_i:start_i+self.pool_size_y,\n",
    "                                  start_j:start_j+self.pool_size_x] += local_mask\n",
    "\n",
    "        return out  # (N, C, H_out, W_out)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        N, C, H, W = self.x.shape\n",
    "        H_out, W_out = grad_output.shape[2], grad_output.shape[3]\n",
    "\n",
    "        # Initialize gradient w.r.t input as float\n",
    "        dx = np.zeros(self.x.shape, dtype=grad_output.dtype)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        start_i = i * self.stride_y\n",
    "                        start_j = j * self.stride_x\n",
    "\n",
    "                        # Use cached mask instead of recomputing\n",
    "                        mask_window = self.mask[n, c, start_i:start_i+self.pool_size_y,\n",
    "                                                start_j:start_j+self.pool_size_x]\n",
    "\n",
    "                        # Distribute gradient to max locations\n",
    "                        dx[n, c, start_i:start_i+self.pool_size_y,\n",
    "                        start_j:start_j+self.pool_size_x] += mask_window * grad_output[n, c, i, j]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f81249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        self.KH, self.KW = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        scale = np.sqrt(2 / (in_channels * self.KH * self.KW))\n",
    "        self.W = nn.Parameter(scale * np.random.randn(out_channels, in_channels, self.KH, self.KW))\n",
    "        self.b = nn.Parameter(np.zeros(out_channels))\n",
    "\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        F_out = self.W.data.shape[0]\n",
    "        KH, KW = self.KH, self.KW\n",
    "        stride, pad = self.stride, self.padding\n",
    "\n",
    "        # Output dimensions\n",
    "        H_out = (H + 2*pad - KH) // stride + 1\n",
    "        W_out = (W + 2*pad - KW) // stride + 1\n",
    "\n",
    "        # Pad input\n",
    "        x_padded = np.pad(x, ((0,0),(0,0),(pad,pad),(pad,pad)), mode=\"constant\")\n",
    "        out = np.zeros((N, F_out, H_out, W_out))\n",
    "\n",
    "        # Sliding-window style\n",
    "        for n in range(N):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    h_start = i * stride\n",
    "                    h_end = h_start + KH\n",
    "                    w_start = j * stride\n",
    "                    w_end = w_start + KW\n",
    "\n",
    "                    # window of shape (C, KH, KW)\n",
    "                    window = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    # Compute convolution for all filters at once\n",
    "                    for f in range(F_out):\n",
    "                        out[n, f, i, j] = np.sum(window * self.W.data[f]) + self.b.data[f]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.x\n",
    "        N, C, H, W = x.shape\n",
    "        F_out = self.W.data.shape[0]\n",
    "        KH, KW = self.KH, self.KW\n",
    "        stride, pad = self.stride, self.padding\n",
    "\n",
    "        H_out, W_out = grad_output.shape[2], grad_output.shape[3]\n",
    "\n",
    "        # Initialize gradients\n",
    "        dx = np.zeros_like(x)\n",
    "        dW = np.zeros_like(self.W.data)\n",
    "        db = np.zeros_like(self.b.data)\n",
    "\n",
    "        # Pad for easy indexing\n",
    "        dx_padded = np.pad(dx, ((0,0),(0,0),(pad,pad),(pad,pad)), mode=\"constant\")\n",
    "        x_padded = np.pad(x, ((0,0),(0,0),(pad,pad),(pad,pad)), mode=\"constant\")\n",
    "\n",
    "        # Sliding-window style backward\n",
    "        for n in range(N):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    h_start = i * stride\n",
    "                    h_end = h_start + KH\n",
    "                    w_start = j * stride\n",
    "                    w_end = w_start + KW\n",
    "\n",
    "                    window = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    for f in range(F_out):\n",
    "                        g = grad_output[n, f, i, j]\n",
    "                        db[f] += g\n",
    "                        dW[f] += g * window\n",
    "                        dx_padded[n, :, h_start:h_end, w_start:w_end] += g * self.W.data[f]\n",
    "\n",
    "        dx = dx_padded[:, :, pad:H+pad, pad:W+pad]\n",
    "\n",
    "        # accumulate gradients in Parameter objects\n",
    "        self.W.grad += dW\n",
    "        self.b.grad += db\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0aeb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output.reshape(self.input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = nn.Sequential([\n",
    "    Conv2d(in_channels=1, out_channels=4, kernel_size=3),  \n",
    "    nn.ReLU(),\n",
    "    Flatten(),                                              \n",
    "    nn.Linear(2704, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e102f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = nn.Sequential([\n",
    "    Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    MaxPool2d(pool_size=(2,2), stride=(2,2)),       # 26â†’13\n",
    "\n",
    "    Flatten(),                                      # 4 * 13 * 13 = 676\n",
    "    nn.Linear(676, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4af491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Train Acc=0.1967, Train Loss=4.2005, Test Acc=0.1900, Test Loss=4.3799\n",
      "Epoch 2 Summary: Train Acc=0.2633, Train Loss=3.3215, Test Acc=0.2300, Test Loss=3.3059\n",
      "Epoch 3 Summary: Train Acc=0.2867, Train Loss=2.8011, Test Acc=0.2800, Test Loss=2.7836\n",
      "Epoch 4 Summary: Train Acc=0.3233, Train Loss=2.4706, Test Acc=0.2900, Test Loss=2.4521\n",
      "Epoch 5 Summary: Train Acc=0.3800, Train Loss=2.2827, Test Acc=0.3300, Test Loss=2.2657\n",
      "Epoch 6 Summary: Train Acc=0.4500, Train Loss=2.1713, Test Acc=0.4200, Test Loss=2.1569\n",
      "Epoch 7 Summary: Train Acc=0.5000, Train Loss=2.0904, Test Acc=0.4900, Test Loss=2.0795\n",
      "Epoch 8 Summary: Train Acc=0.5200, Train Loss=2.0166, Test Acc=0.5400, Test Loss=2.0092\n",
      "Epoch 9 Summary: Train Acc=0.5567, Train Loss=1.9428, Test Acc=0.5600, Test Loss=1.9389\n",
      "Epoch 10 Summary: Train Acc=0.5733, Train Loss=1.8672, Test Acc=0.5600, Test Loss=1.8670\n",
      "Epoch 11 Summary: Train Acc=0.5733, Train Loss=1.7894, Test Acc=0.5600, Test Loss=1.7926\n",
      "Epoch 12 Summary: Train Acc=0.5800, Train Loss=1.7116, Test Acc=0.5600, Test Loss=1.7180\n",
      "Epoch 13 Summary: Train Acc=0.5933, Train Loss=1.6363, Test Acc=0.5900, Test Loss=1.6457\n",
      "Epoch 14 Summary: Train Acc=0.6033, Train Loss=1.5628, Test Acc=0.5800, Test Loss=1.5748\n",
      "Epoch 15 Summary: Train Acc=0.6133, Train Loss=1.4935, Test Acc=0.5800, Test Loss=1.5080\n"
     ]
    }
   ],
   "source": [
    "model = model_1\n",
    "\n",
    "loss_fn = CrossEntropy()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "initial_lr = 0.01\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        logits = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "        grad_output = loss_fn.backward()\n",
    "        model.backward(grad_output)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    logits_train = model.forward(X_train)\n",
    "    train_loss = loss_fn.forward(logits_train, y_train)\n",
    "    train_acc = accuracy(logits_train, y_train)\n",
    "\n",
    "    logits_test = model.forward(X_test)\n",
    "    test_loss = loss_fn.forward(logits_test, y_test)\n",
    "    test_acc = accuracy(logits_test, y_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: \"\n",
    "          f\"Train Acc={train_acc:.4f}, Train Loss={train_loss:.4f}, \"\n",
    "          f\"Test Acc={test_acc:.4f}, Test Loss={test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
