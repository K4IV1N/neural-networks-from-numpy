{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb489d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c79b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        \n",
    "class Module:\n",
    "    # ADD\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    # ADD\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Parameter):\n",
    "            self.params[name] = value\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = list(self.params.values())\n",
    "        for attr in self.__dict__.values():\n",
    "            if isinstance(attr, Module):\n",
    "                params.extend(attr.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # Normal mode: user specifies input and output size\n",
    "        if len(args) == 2:  # example: Linear(128, 32)\n",
    "            in_features, out_features = args\n",
    "            self.deferred_init = False\n",
    "            self.initialize_params(in_features, out_features)\n",
    "\n",
    "        # Deferred initialization: Linear(32)\n",
    "        elif len(args) == 1:\n",
    "            (out_features,) = args\n",
    "            self.deferred_init = True\n",
    "            self.out_features = out_features\n",
    "            self.W = None\n",
    "            self.b = None\n",
    "        else:\n",
    "            raise ValueError(\"Linear expects 1 or 2 arguments\")\n",
    "\n",
    "\n",
    "    def initialize_params(self, in_features, out_features):\n",
    "        # simple \n",
    "        self.W = Parameter(np.random.randn(in_features, out_features) * 0.01)   # (in_features, out_features, )\n",
    "\n",
    "        # Kaiming He normal initialization (best for ReLU networks)\n",
    "        # std = np.sqrt(2.0 / in_features)                    \n",
    "        # self.W = Parameter(np.random.randn(in_features, out_features) * std)  # (in_features, out_features, )\n",
    "        \n",
    "        self.b = Parameter(np.zeros(out_features)) # (output_features,)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Deferred initialization\n",
    "        if self.deferred_init and self.W is None:\n",
    "            in_features = x.shape[-1]\n",
    "            self.initialize_params(in_features, self.out_features)\n",
    "            self.deferred_init = False\n",
    "\n",
    "        self.x = x\n",
    "        # x: (batch, in_features) \n",
    "        return x @ self.W.data + self.b.data    # (batch, out_features)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.W.grad += self.x.T @ grad_output      # (in, batch) @ (batch, out) → (in, out)\n",
    "        self.b.grad += grad_output.sum(axis=0)\n",
    "        return grad_output @ self.W.data.T          # (batch, out) @ (out, in) → (batch, in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dd1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad_input):\n",
    "        return grad_input * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c4f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred  # Store predictions for backward pass\n",
    "\n",
    "        # Convert 1D class labels to one-hot if needed\n",
    "        if y_true.ndim == 1:\n",
    "            num_classes = y_pred.shape[1]\n",
    "            self.y_true = np.eye(num_classes)[y_true]\n",
    "        else:\n",
    "            self.y_true = y_true  # Already in proper shape\n",
    "\n",
    "        # Match dtype with predictions\n",
    "        self.y_true = self.y_true.astype(y_pred.dtype)\n",
    "\n",
    "        # Average of squared differences\n",
    "        loss = np.mean((y_pred - self.y_true) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self): # dL/dY_pred\n",
    "        return 2 * (self.y_pred - self.y_true) / self.y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948015dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred # Store predictions for backward pass\n",
    "\n",
    "        # If labels are 1D (class indices), convert them to one-hot encoding\n",
    "        if y_true.ndim == 1:\n",
    "            num_classes = y_pred.shape[1]  # Number of output classes\n",
    "            self.y_true = np.eye(num_classes)[y_true]  # One-hot encode\n",
    "        else:\n",
    "            self.y_true = y_true  # Already one-hot encoded\n",
    "\n",
    "        # Match dtype with predictions\n",
    "        self.y_true = self.y_true.astype(y_pred.dtype)\n",
    "\n",
    "        # Clip predictions to avoid log(0) which can cause numerical issues\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "\n",
    "        # Compute cross-entropy loss:\n",
    "        #   - sum over classes for each sample\n",
    "        #   - then average over all samples\n",
    "        loss = -np.mean(np.sum(self.y_true * np.log(y_pred_clipped), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        N = self.y_pred.shape[0]  # batch size\n",
    "        # Gradient: (y_pred - y_true) / N\n",
    "        grad = (self.y_pred - self.y_true) / N\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430cad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, _module, lr=0.01):\n",
    "        # Check if the input _module is an instance of the Module class\n",
    "        # (e.g., Linear, Activation layers)\n",
    "        if isinstance(_module, Module):  # If it's a Module, get all the parameters (weights and biases)\n",
    "            self._module = _module.parameters()\n",
    "        else:\n",
    "            # If it's already a list of parameters, assign it directly\n",
    "            self._module = _module\n",
    "        self.lr = lr  # Set learning rate (default 0.01)\n",
    "\n",
    "    def step(self):\n",
    "        # Loop through all weights and biases of the model\n",
    "        for param in self._module:\n",
    "            # Update each weights and biases using SGD formula\n",
    "            param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self._module:\n",
    "            param.grad[...] = 0  # reset their gradients to zero to prevent accumulation from previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63749f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return np.mean(preds == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 2 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 3 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 4 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 5 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 6 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 7 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 8 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 9 Summary: Train Acc=0.2000, Train Loss=0.8943\n",
      "Epoch 10 Summary: Train Acc=0.2000, Train Loss=0.8943\n"
     ]
    }
   ],
   "source": [
    "class MyModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(784, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(128, 32)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_output = self.fc3.backward(grad_output)\n",
    "        grad_output = self.relu2.backward(grad_output)\n",
    "        grad_output = self.fc2.backward(grad_output)\n",
    "        grad_output = self.relu1.backward(grad_output)\n",
    "        grad_output = self.fc1.backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "epochs = 10         # Number of epochs\n",
    "batch_size = 10     # Number of samples\n",
    "num_classes = 10    # Number of class\n",
    "\n",
    "# loss_fn = MSE()\n",
    "loss_fn = CrossEntropy()\n",
    "\n",
    "initial_lr = 0.01\n",
    "optimizer = SGD(model, lr=initial_lr)\n",
    "\n",
    "# input (random images)\n",
    "X_random = np.random.rand(batch_size, 784)   # shape: (10,784)\n",
    "\n",
    "# labels (random class indices 0–9)\n",
    "y_indices = np.random.randint(0, num_classes, size=batch_size)\n",
    "\n",
    "# y_batch = np.eye(num_classes)[y_indices]\n",
    "# Not needed because we defined the loss to handle both label encoding and one-hot encoding.\n",
    "\n",
    "model = MyModel()   # Instantiate the model\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_random.shape[0], batch_size):\n",
    "        # Get the current batch\n",
    "        x_batch = X_random[i:i+batch_size]\n",
    "        y_batch = y_indices[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        grad_output = loss_fn.backward()  # Get gradient of the loss\n",
    "        model.backward(grad_output)  # Propagate gradients back through the model\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        iter_num = i // batch_size + 1\n",
    "\n",
    "    # Evaluate on the entire training set\n",
    "    logits_train = model.forward(X_random)\n",
    "    train_loss = loss_fn.forward(logits_train, y_indices)\n",
    "    train_acc = accuracy(logits_train, y_indices)\n",
    "\n",
    "    # Print progress for the current epoch\n",
    "    print(f\"Epoch {epoch+1} Summary: \"\n",
    "          f\"Train Acc={train_acc:.4f}, Train Loss={train_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
