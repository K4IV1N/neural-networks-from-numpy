{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb489d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52516b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(loader_fn, train_num, test_num):\n",
    "    data_x, data_y = loader_fn\n",
    "    \n",
    "    classes = np.unique(data_y)\n",
    "    \n",
    "    train_x_list = []\n",
    "    train_y_list = []\n",
    "    test_x_list = []\n",
    "    test_y_list = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_indices = np.where(data_y == cls)[0]\n",
    "        cls_indices = np.random.permutation(cls_indices)\n",
    "        \n",
    "        X_cls = data_x[cls_indices]\n",
    "        Y_cls = data_y[cls_indices]\n",
    "        \n",
    "        train_x_list.append(X_cls[:train_num])\n",
    "        train_y_list.append(Y_cls[:train_num])\n",
    "        \n",
    "        test_x_list.append(X_cls[train_num:train_num + test_num])\n",
    "        test_y_list.append(Y_cls[train_num:train_num + test_num])\n",
    "        \n",
    "    X_train = np.concatenate(train_x_list)\n",
    "    y_train = np.concatenate(train_y_list)\n",
    "    X_test = np.concatenate(test_x_list)\n",
    "    y_test = np.concatenate(test_y_list)\n",
    "    \n",
    "    train_perm = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[train_perm]\n",
    "    y_train = y_train[train_perm]\n",
    "    \n",
    "    test_perm = np.random.permutation(len(X_test))\n",
    "    X_test = X_test[test_perm]\n",
    "    y_test = y_test[test_perm]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c789078",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e04c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "# Normalize pixel values to range 0-1 by dividing by 255.0 (grayscale images originally 0-255)\n",
    "# Convert labels to integer type using astype('int16') to ensure numeric operations work correctly \n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset(\n",
    "    (\n",
    "        np.asarray(data[\"data\"].values) / 255.0,                 # Normalize input images\n",
    "        np.asarray(data[\"target\"].values.astype('int16'))        # Convert labels to integers\n",
    "    ),\n",
    "    train_num=3000,\n",
    "    test_num=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c79b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        \n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.layer_dict = {}     # Add\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Parameter):\n",
    "            self.params[name] = value\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "        if isinstance(value, Module):\n",
    "            self.layer_dict[name] = value\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = list(self.params.values())\n",
    "        for attr in self.__dict__.values():\n",
    "            if isinstance(attr, Module):\n",
    "                params.extend(attr.parameters())\n",
    "        return params\n",
    "\n",
    "    # Add\n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(list(self.layer_dict.values())):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113f29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # Normal mode: user specifies input and output size\n",
    "        if len(args) == 2:  # example: Linear(128, 32)\n",
    "            in_features, out_features = args\n",
    "            self.deferred_init = False\n",
    "            self.initialize_params(in_features, out_features)\n",
    "\n",
    "        # Deferred initialization: Linear(32)\n",
    "        elif len(args) == 1:\n",
    "            (out_features,) = args\n",
    "            self.deferred_init = True\n",
    "            self.out_features = out_features\n",
    "            self.W = None\n",
    "            self.b = None\n",
    "        else:\n",
    "            raise ValueError(\"Linear expects 1 or 2 arguments\")\n",
    "\n",
    "\n",
    "    def initialize_params(self, in_features, out_features):\n",
    "        # simple \n",
    "        self.W = Parameter(np.random.randn(in_features, out_features) * 0.01)   # (in_features, out_features, )\n",
    "\n",
    "        # Kaiming He normal initialization (best for ReLU networks)\n",
    "        # std = np.sqrt(2.0 / in_features)                    \n",
    "        # self.W = Parameter(np.random.randn(in_features, out_features) * std)  # (in_features, out_features, )\n",
    "        \n",
    "        self.b = Parameter(np.zeros(out_features)) # (output_features,)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Deferred initialization\n",
    "        if self.deferred_init and self.W is None:\n",
    "            in_features = x.shape[-1]\n",
    "            self.initialize_params(in_features, self.out_features)\n",
    "            self.deferred_init = False\n",
    "\n",
    "        self.x = x\n",
    "        # x: (batch, in_features) \n",
    "        return x @ self.W.data + self.b.data    # (batch, out_features)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.W.grad += self.x.T @ grad_output      # (in, batch) @ (batch, out) → (in, out)\n",
    "        self.b.grad += grad_output.sum(axis=0)\n",
    "        return grad_output @ self.W.data.T          # (batch, out) @ (out, in) → (batch, in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8dd1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad_input):\n",
    "        return grad_input * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c4f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred  # Store predictions for backward pass\n",
    "\n",
    "        # Convert 1D class labels to one-hot if needed\n",
    "        if y_true.ndim == 1:\n",
    "            num_classes = y_pred.shape[1]\n",
    "            self.y_true = np.eye(num_classes)[y_true]\n",
    "        else:\n",
    "            self.y_true = y_true  # Already in proper shape\n",
    "\n",
    "        # Match dtype with predictions\n",
    "        self.y_true = self.y_true.astype(y_pred.dtype)\n",
    "\n",
    "        # Average of squared differences\n",
    "        loss = np.mean((y_pred - self.y_true) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self): # dL/dY_pred\n",
    "        return 2 * (self.y_pred - self.y_true) / self.y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430cad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, _module, lr=0.01):\n",
    "        # Check if the input _module is an instance of the Module class\n",
    "        # (e.g., Linear, Activation layers)\n",
    "        if isinstance(_module, Module):  # If it's a Module, get all the parameters (weights and biases)\n",
    "            self._module = _module.parameters()\n",
    "        else:\n",
    "            # If it's already a list of parameters, assign it directly\n",
    "            self._module = _module\n",
    "        self.lr = lr  # Set learning rate (default 0.01)\n",
    "\n",
    "    def step(self):\n",
    "        # Loop through all weights and biases of the model\n",
    "        for param in self._module:\n",
    "            # Update each weights and biases using SGD formula\n",
    "            param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self._module:\n",
    "            param.grad[...] = 0  # reset their gradients to zero to prevent accumulation from previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63749f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return np.mean(preds == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a10e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layer_dict = {}          # Store the layers\n",
    "        for i, layer in enumerate(layers):\n",
    "            self.layer_dict['layer' + str(i)] = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each layer in order\n",
    "        for i in sorted(self.layer_dict.keys()):\n",
    "            x = self.layer_dict[i](x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Backpropagate through each layer in reverse order\n",
    "        for i in reversed(sorted(self.layer_dict.keys())):\n",
    "            grad_output = self.layer_dict[i].backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "    def parameters(self):\n",
    "        # Collect parameters from all layers\n",
    "        params = []\n",
    "        for layer in self.layer_dict.values():\n",
    "            if hasattr(layer, 'parameters'):\n",
    "                params.extend(layer.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d93ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = Sequential([\n",
    "    Linear(784, 128),\n",
    "    ReLU(),\n",
    "    Linear(32),\n",
    "    ReLU(),\n",
    "    Linear(10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4eca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(784, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(128, 32)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model_custom = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Train Acc=0.1661, Train Loss=0.0900, Test Acc=0.1670, Test Loss=0.0900\n",
      "Epoch 2 Summary: Train Acc=0.1873, Train Loss=0.0898, Test Acc=0.1920, Test Loss=0.0898\n",
      "Epoch 3 Summary: Train Acc=0.2871, Train Loss=0.0888, Test Acc=0.2960, Test Loss=0.0888\n",
      "Epoch 4 Summary: Train Acc=0.3243, Train Loss=0.0809, Test Acc=0.3170, Test Loss=0.0809\n",
      "Epoch 5 Summary: Train Acc=0.5413, Train Loss=0.0698, Test Acc=0.5430, Test Loss=0.0699\n",
      "Epoch 6 Summary: Train Acc=0.6260, Train Loss=0.0646, Test Acc=0.6500, Test Loss=0.0647\n"
     ]
    }
   ],
   "source": [
    "loss_fn = MSE()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "initial_lr = 0.01\n",
    "\n",
    "# model = model_seq\n",
    "model = model_custom \n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        logits = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "        grad_output = loss_fn.backward()\n",
    "        model.backward(grad_output)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    logits_train = model.forward(X_train)\n",
    "    train_loss = loss_fn.forward(logits_train, y_train)\n",
    "    train_acc = accuracy(logits_train, y_train)\n",
    "\n",
    "    logits_test = model.forward(X_test)\n",
    "    test_loss = loss_fn.forward(logits_test, y_test)\n",
    "    test_acc = accuracy(logits_test, y_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: \"\n",
    "          f\"Train Acc={train_acc:.4f}, Train Loss={train_loss:.4f}, \"\n",
    "          f\"Test Acc={test_acc:.4f}, Test Loss={test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd99da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    state = {}\n",
    "    for module_name, module in model.layer_dict.items():\n",
    "        if isinstance(module, Module):\n",
    "            for pname, p in module.params.items():\n",
    "                state[f\"{module_name}.{pname}\"] = p.data\n",
    "\n",
    "    for k, v in state.items():\n",
    "        print(f\"{k}: shape={v.shape}\")\n",
    "    np.save(path, state)\n",
    "\n",
    "def load_model(model, path):\n",
    "    # Load the saved file\n",
    "    state = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "    # Update the parameters of the current model\n",
    "    for module_name, module in model.layer_dict.items():\n",
    "        if isinstance(module, Module):\n",
    "            for pname, p in module.params.items():\n",
    "                key = f\"{module_name}.{pname}\"\n",
    "                if key in state:\n",
    "                    p.data = state[key]  # Update its value\n",
    "                else:\n",
    "                    print(\"Missing:\", key)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.W: shape=(784, 128)\n",
      "fc1.b: shape=(128,)\n",
      "fc2.W: shape=(128, 32)\n",
      "fc2.b: shape=(32,)\n",
      "fc3.W: shape=(32, 10)\n",
      "fc3.b: shape=(10,)\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "save_path = 'saved_1.npy'\n",
    "save_model(model, save_path)\n",
    "\n",
    "# load\n",
    "model_load = load_model(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6617cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Train Acc=0.9104, Train Loss=0.0218, Test Acc=0.9030, Test Loss=0.0228\n",
      "Epoch 2 Summary: Train Acc=0.9158, Train Loss=0.0205, Test Acc=0.9100, Test Loss=0.0215\n",
      "Epoch 3 Summary: Train Acc=0.9207, Train Loss=0.0193, Test Acc=0.9140, Test Loss=0.0203\n",
      "Epoch 4 Summary: Train Acc=0.9247, Train Loss=0.0183, Test Acc=0.9210, Test Loss=0.0192\n",
      "Epoch 5 Summary: Train Acc=0.9297, Train Loss=0.0174, Test Acc=0.9240, Test Loss=0.0183\n"
     ]
    }
   ],
   "source": [
    "loss_fn = MSE()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "initial_lr = 0.01\n",
    "\n",
    "# Reinitialize optimizer after loading the model\n",
    "optimizer = SGD(model_load.parameters(), lr=initial_lr)\n",
    "\n",
    "# Continue with the training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        logits = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "        grad_output = loss_fn.backward()\n",
    "        model.backward(grad_output)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    logits_train = model.forward(X_train)\n",
    "    train_loss = loss_fn.forward(logits_train, y_train)\n",
    "    train_acc = accuracy(logits_train, y_train)\n",
    "\n",
    "    logits_test = model.forward(X_test)\n",
    "    test_loss = loss_fn.forward(logits_test, y_test)\n",
    "    test_acc = accuracy(logits_test, y_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: \"\n",
    "          f\"Train Acc={train_acc:.4f}, Train Loss={train_loss:.4f}, \"\n",
    "          f\"Test Acc={test_acc:.4f}, Test Loss={test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
